{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac7e4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "89d9f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a7fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType,StructField,StructType,DateType,DoubleType\n",
    "\n",
    "schema = StructType([StructField(\"Index\", IntegerType(), True),\n",
    "\tStructField(\"Date\", DateType(), True),\n",
    "\tStructField(\"Time\", StringType(), True),\n",
    "\tStructField(\"Total_Precipitation\", DoubleType(), True),\n",
    "\tStructField(\"Atmospheric_Pressure\", DoubleType(), True),\n",
    "\tStructField(\"Maximum_Atmospheric_Pressure\", DoubleType(), True),\n",
    "\tStructField(\"Minimum_Atmospheric_Pressure\", DoubleType(), True),\n",
    "\tStructField(\"Solar_Radiation\", DoubleType(), True),\n",
    "\tStructField(\"Air_Temperature\", DoubleType(), True),\n",
    "\tStructField(\"Dew_Point_Temperature\", DoubleType(), True),\n",
    "\tStructField(\"Maximum_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Minimum_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Maximum_Air_Pressure_Dew_Point_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Minimum_Air_Pressure_Dew_Point_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", DoubleType(), True),\t\n",
    "\tStructField(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Air_Relative_Humid_Temperature_For_The_Last_Hour\", DoubleType(), True),\n",
    "\tStructField(\"Wind_Direction_Radius_Degree\", DoubleType(), True),\n",
    "\tStructField(\"Wind_Gust_In_Metres_Per_Second\", DoubleType(), True),\n",
    "\tStructField(\"Wind_speed_In_Metres_Per_Second\", DoubleType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "\tStructField(\"State\", StringType(), True),\n",
    "\tStructField(\"Station_Name\", StringType(), True),\n",
    "\tStructField(\"Station_Code\", StringType(), True),\n",
    "\tStructField(\"Latitude\", DoubleType(), True),\n",
    "\tStructField(\"Longitude\", DoubleType(), True),\n",
    "\tStructField(\"Elevation\", DoubleType(), True)\n",
    "\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17b1f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"file:///home/talentum/shared/CDAC_Project_Weather_Anaysis/final_north_weather_data.csv\"\n",
    "df1 = spark.read.csv(file_location,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1724e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|      Date| Time|Total_Precipitation|Atmospheric_Pressure|Solar_Radiation|Maximum_Temperature_For_The_Last_Hour|Minimum_Temperature_For_The_Last_Hour|Maximum_Relative_Humid_Temperature_For_The_Last_Hour|Minimum_Relative_Humid_Temperature_For_The_Last_Hour|Wind_speed_In_Metres_Per_Second|Region|State|Station_Name|Station_Code|   Latitude|   Longitude|Elevation|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|2000-07-03|11:00|                0.0|              1005.4|           59.0|                                 26.0|                                 25.5|                                                90.0|                                                88.0|                            1.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|12:00|                0.0|              1006.1|          346.0|                                 26.6|                                 26.0|                                                90.0|                                                87.0|                            1.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|13:00|                0.0|              1006.6|          863.0|                                 27.5|                                 26.5|                                                88.0|                                                83.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|14:00|                0.0|              1006.8|         1481.0|                                 29.1|                                 27.5|                                                83.0|                                                76.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|15:00|                0.0|              1006.7|         2337.0|                                 30.0|                                 28.0|                                                80.0|                                                70.0|                            2.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|16:00|                0.0|              1006.4|         2044.0|                                 29.7|                                 28.8|                                                77.0|                                                70.0|                            1.9|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|18:00|                0.0|              1003.5|         2988.0|                                 31.7|                                 31.0|                                                65.0|                                                60.0|                            3.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|19:00|                0.0|              1002.3|         2362.0|                                 31.9|                                 30.3|                                                64.0|                                                58.0|                            2.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|20:00|                0.0|              1001.5|         1560.0|                                 31.2|                                 30.3|                                                64.0|                                                61.0|                            2.2|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|21:00|                0.0|              1001.7|          995.0|                                 30.7|                                 29.6|                                                67.0|                                                64.0|                            2.1|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|22:00|                0.0|              1002.6|          105.0|                                 29.6|                                 27.7|                                                78.0|                                                65.0|                            0.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|10:00|                0.0|              1003.4|            1.0|                                 24.5|                                 24.2|                                                95.0|                                                94.0|                            1.4|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|11:00|                0.0|              1004.3|           84.0|                                 24.8|                                 24.4|                                                94.0|                                                93.0|                            0.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|12:00|                0.0|              1004.9|          691.0|                                 26.3|                                 24.7|                                                94.0|                                                85.0|                            2.1|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|13:00|                0.2|              1005.2|         1922.0|                                 28.4|                                 26.3|                                                85.0|                                                75.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|14:00|                0.0|              1005.2|         2444.0|                                 29.1|                                 28.2|                                                80.0|                                                74.0|                            2.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|15:00|                0.0|              1004.3|         3262.0|                                 30.2|                                 28.9|                                                76.0|                                                69.0|                            2.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|16:00|                0.0|              1003.2|         2916.0|                                 31.2|                                 29.6|                                                72.0|                                                65.0|                            2.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|17:00|                0.0|              1002.3|         2963.0|                                 31.5|                                 29.9|                                                70.0|                                                63.0|                            2.2|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|18:00|                0.0|              1001.6|         2590.0|                                 31.6|                                 29.6|                                                70.0|                                                60.0|                            3.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = [Row(Name=\"David\", Age=28), Row(Name=\"Eva\", Age=32)]\n",
    "\n",
    "# Convert the new rows to a DataFrame\n",
    "new_df = spark.createDataFrame(new_rows, columns)\n",
    "final_df = df.union(new_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfddbb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d48faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001.3\n",
      "0.0\n",
      "1723.0\n",
      "33.0\n",
      "31.3\n",
      "59.0\n",
      "52.0\n",
      "2.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "\n",
    "df1 = df1.withColumn(\"Total_Precipitation\", F.col(\"Total_Precipitation\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Atmospheric_Pressure\", F.col(\"Atmospheric_Pressure\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Solar_Radiation\", F.col(\"Solar_Radiation\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Maximum_Temperature_For_The_Last_Hour\", F.col(\"Maximum_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Minimum_Temperature_For_The_Last_Hour\", F.col(\"Minimum_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", F.col(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", F.col(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Wind_speed_In_Metres_Per_Second\", F.col(\"Wind_speed_In_Metres_Per_Second\").cast(DoubleType()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = df1.filter((F.col(\"Date\") >= \"2009-10-21\") & (F.col(\"Date\") <= \"2009-10-29\"))\n",
    "#filtered_df.show(5)\n",
    "median_value_Atmos_Pressure = filtered_df.approxQuantile(\"Atmospheric_Pressure\", [0.5], 0.01)[0]\n",
    "median_value_Solar_Rad = filtered_df.approxQuantile(\"Solar_Radiation\", [0.5], 0.01)[0]\n",
    "median_value_Max_Temp = filtered_df.approxQuantile(\"Maximum_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Min_Temp = filtered_df.approxQuantile(\"Minimum_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Max_Humid = filtered_df.approxQuantile(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Min_Humid = filtered_df.approxQuantile(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Wind_Speed = filtered_df.approxQuantile(\"Wind_speed_In_Metres_Per_Second\", [0.5], 0.01)[0]\n",
    "\n",
    "\n",
    "mean_value_Precipitation = filtered_df.agg(mean(\"Total_Precipitation\")).collect()[0][0]\n",
    "print(median_value_Atmos_Pressure)\n",
    "print(mean_value_Precipitation)\n",
    "print(median_value_Solar_Rad)\n",
    "print(median_value_Max_Temp)\n",
    "print(median_value_Min_Temp)\n",
    "print(median_value_Max_Humid)\n",
    "print(median_value_Min_Humid)\n",
    "print(median_value_Wind_Speed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1eaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a95f60ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|      Date| Time|Total_Precipitation|Atmospheric_Pressure|Solar_Radiation|Maximum_Temperature_For_The_Last_Hour|Minimum_Temperature_For_The_Last_Hour|Maximum_Relative_Humid_Temperature_For_The_Last_Hour|Minimum_Relative_Humid_Temperature_For_The_Last_Hour|Wind_speed_In_Metres_Per_Second|Region|State|Station_Name|Station_Code|   Latitude|   Longitude|Elevation|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|2009-07-15|11:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|12:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|13:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|14:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|15:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|16:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|17:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|18:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|19:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|20:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|21:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-07-15|22:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|11:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|12:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|13:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|14:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|15:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|16:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|17:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2009-02-01|18:00|                0.0|              1001.3|         1723.0|                                 33.0|                                 31.3|                                                59.0|                                                 2.6|                            0.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "\n",
    "df1 = df1.withColumn(\"Total_Precipitation\", F.col(\"Total_Precipitation\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Atmospheric_Pressure\", F.col(\"Atmospheric_Pressure\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Solar_Radiation\", F.col(\"Solar_Radiation\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Maximum_Temperature_For_The_Last_Hour\", F.col(\"Maximum_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Minimum_Temperature_For_The_Last_Hour\", F.col(\"Minimum_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", F.col(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", F.col(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\").cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"Wind_speed_In_Metres_Per_Second\", F.col(\"Wind_speed_In_Metres_Per_Second\").cast(DoubleType()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = df1.filter((F.col(\"Date\") >= \"2009-10-21\") & (F.col(\"Date\") <= \"2009-10-29\"))\n",
    "#filtered_df.show(5)\n",
    "median_value_Atmos_Pressure = filtered_df.approxQuantile(\"Atmospheric_Pressure\", [0.5], 0.01)[0]\n",
    "median_value_Solar_Rad = filtered_df.approxQuantile(\"Solar_Radiation\", [0.5], 0.01)[0]\n",
    "median_value_Max_Temp = filtered_df.approxQuantile(\"Maximum_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Min_Temp = filtered_df.approxQuantile(\"Minimum_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Max_Humid = filtered_df.approxQuantile(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Min_Humid = filtered_df.approxQuantile(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", [0.5], 0.01)[0]\n",
    "median_value_Wind_Speed = filtered_df.approxQuantile(\"Wind_speed_In_Metres_Per_Second\", [0.5], 0.01)[0]\n",
    "\n",
    "\n",
    "mean_value_Precipitation = filtered_df.agg(mean(\"Total_Precipitation\")).collect()[0][0]\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.sql.functions import lit\n",
    "df2 = df1.filter((F.col(\"Date\") >= \"2008-01-01\") & (F.col(\"Date\") <= \"2008-10-20\"))\n",
    "#df1 = df1.withColumn(\"Date\", when((df1[\"Date\"] == \"2008-01-01\"), \"2009-01-01\").otherwise(df1[\"Age\"]))\n",
    "#df1.show(5)\n",
    "\n",
    "\n",
    "condition = (col(\"Date\").startswith(\"2008\"))\n",
    "\n",
    "# Update the \"Date\" column by changing the year from 2008 to 2009\n",
    "df3 = df2.withColumn(\"Date\", when(condition, expr(\"to_date('2009-' || substr(Date, 6))\")).otherwise(col(\"Date\")))\n",
    "#df3 = df2.withColumn(\"Atmospheric_Pressure\", when(condition, expr(\"to_date('2009-' || substr(Date, 6))\")).otherwise(col(\"Date\")))\n",
    "df3 = df3.withColumn(\"Atmospheric_Pressure\", lit(median_value_Atmos_Pressure))\n",
    "df3 = df3.withColumn(\"Solar_Radiation\", lit(median_value_Solar_Rad))\n",
    "df3 = df3.withColumn(\"Maximum_Temperature_For_The_Last_Hour\", lit(median_value_Max_Temp))\n",
    "df3 = df3.withColumn(\"Minimum_Temperature_For_The_Last_Hour\", lit(median_value_Min_Temp))\n",
    "df3 = df3.withColumn(\"Maximum_Relative_Humid_Temperature_For_The_Last_Hour\", lit(median_value_Max_Humid))\n",
    "df3 = df3.withColumn(\"Minimum_Relative_Humid_Temperature_For_The_Last_Hour\", lit(median_value_Wind_Speed))\n",
    "df3 = df3.withColumn(\"Wind_speed_In_Metres_Per_Second\", lit(mean_value_Precipitation))\n",
    "\n",
    "df3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db9426b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2818"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "75bd0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|      Date| Time|Total_Precipitation|Atmospheric_Pressure|Solar_Radiation|Maximum_Temperature_For_The_Last_Hour|Minimum_Temperature_For_The_Last_Hour|Maximum_Relative_Humid_Temperature_For_The_Last_Hour|Minimum_Relative_Humid_Temperature_For_The_Last_Hour|Wind_speed_In_Metres_Per_Second|Region|State|Station_Name|Station_Code|   Latitude|   Longitude|Elevation|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "|2000-07-03|11:00|                0.0|              1005.4|           59.0|                                 26.0|                                 25.5|                                                90.0|                                                88.0|                            1.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|12:00|                0.0|              1006.1|          346.0|                                 26.6|                                 26.0|                                                90.0|                                                87.0|                            1.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|13:00|                0.0|              1006.6|          863.0|                                 27.5|                                 26.5|                                                88.0|                                                83.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|14:00|                0.0|              1006.8|         1481.0|                                 29.1|                                 27.5|                                                83.0|                                                76.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|15:00|                0.0|              1006.7|         2337.0|                                 30.0|                                 28.0|                                                80.0|                                                70.0|                            2.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|16:00|                0.0|              1006.4|         2044.0|                                 29.7|                                 28.8|                                                77.0|                                                70.0|                            1.9|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|18:00|                0.0|              1003.5|         2988.0|                                 31.7|                                 31.0|                                                65.0|                                                60.0|                            3.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|19:00|                0.0|              1002.3|         2362.0|                                 31.9|                                 30.3|                                                64.0|                                                58.0|                            2.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|20:00|                0.0|              1001.5|         1560.0|                                 31.2|                                 30.3|                                                64.0|                                                61.0|                            2.2|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|21:00|                0.0|              1001.7|          995.0|                                 30.7|                                 29.6|                                                67.0|                                                64.0|                            2.1|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-07-03|22:00|                0.0|              1002.6|          105.0|                                 29.6|                                 27.7|                                                78.0|                                                65.0|                            0.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|10:00|                0.0|              1003.4|            1.0|                                 24.5|                                 24.2|                                                95.0|                                                94.0|                            1.4|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|11:00|                0.0|              1004.3|           84.0|                                 24.8|                                 24.4|                                                94.0|                                                93.0|                            0.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|12:00|                0.0|              1004.9|          691.0|                                 26.3|                                 24.7|                                                94.0|                                                85.0|                            2.1|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|13:00|                0.2|              1005.2|         1922.0|                                 28.4|                                 26.3|                                                85.0|                                                75.0|                            1.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|14:00|                0.0|              1005.2|         2444.0|                                 29.1|                                 28.2|                                                80.0|                                                74.0|                            2.0|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|15:00|                0.0|              1004.3|         3262.0|                                 30.2|                                 28.9|                                                76.0|                                                69.0|                            2.6|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|16:00|                0.0|              1003.2|         2916.0|                                 31.2|                                 29.6|                                                72.0|                                                65.0|                            2.7|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|17:00|                0.0|              1002.3|         2963.0|                                 31.5|                                 29.9|                                                70.0|                                                63.0|                            2.2|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "|2000-12-26|18:00|                0.0|              1001.6|         2590.0|                                 31.6|                                 29.6|                                                70.0|                                                60.0|                            3.3|     N|   AM|      MANAUS|        A101|-3.10333333|-60.01638888|    61.25|\n",
      "+----------+-----+-------------------+--------------------+---------------+-------------------------------------+-------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------------------------+------+-----+------------+------------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = df1.union(df3)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c704da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84428\n",
      "2818\n",
      "87246\n"
     ]
    }
   ],
   "source": [
    "print(df1.count())\n",
    "print(df3.count())\n",
    "print(final_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "179172d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"final_weather_data_north3\"\n",
    "df_transformed_single_partition = final_df.coalesce(1)\n",
    "\n",
    "# Write the DataFrame to a single CSV file\n",
    "df_transformed_single_partition.write.mode(\"overwrite\").csv(output_csv_path, header=True, compression=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4145379",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.sql.\n: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\t... 74 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 89 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 95 more\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\n\t... 103 more\n)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 100 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-4a7a9e8395e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Extract the median value from the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'"
     ]
    }
   ],
   "source": [
    "# df1 = df1.withColumn(\"Total_Precipitation\", F.col(\"Total_Precipitation\").cast(DoubleType()))\n",
    "# df1.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# query = f\"\"\"\n",
    "#     SELECT approx_percentile(Total_Precipitation, 0.5, 0.01) as median_col1\n",
    "#     FROM my_table\n",
    "#     WHERE Date BETWEEN \"2009-10-21\" AND \"2009-10-29\"\n",
    "# \"\"\"\n",
    "\n",
    "# result = spark.sql(query)\n",
    "\n",
    "# # Extract the median value from the result\n",
    "# median_col1 = result.collect()[0][\"median_col1\"]\n",
    "# print(median_col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a7de93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Total_Precipitation: string (nullable = true)\n",
      " |-- Atmospheric_Pressure: string (nullable = true)\n",
      " |-- Solar_Radiation: string (nullable = true)\n",
      " |-- Maximum_Temperature_For_The_Last_Hour: string (nullable = true)\n",
      " |-- Minimum_Temperature_For_The_Last_Hour: string (nullable = true)\n",
      " |-- Maximum_Relative_Humid_Temperature_For_The_Last_Hour: string (nullable = true)\n",
      " |-- Minimum_Relative_Humid_Temperature_For_The_Last_Hour: string (nullable = true)\n",
      " |-- Wind_speed_In_Metres_Per_Second: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Station_Name: string (nullable = true)\n",
      " |-- Station_Code: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Elevation: string (nullable = true)\n",
      " |-- Date1: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2668af85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35351c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c026a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a9b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a345cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7dfe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332d886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495cba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d6bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
